{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f279ce28-8ea8-4ccd-9c63-b095228e175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autoencoder\n",
    "import utils\n",
    "import mrrmse\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import random\n",
    "\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from ray import train, tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.hyperopt import HyperOptSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771a0f9e-ed59-4568-b6e8-4599c153eb0d",
   "metadata": {},
   "source": [
    "## Prepare data:\n",
    "#### Read joined data (pre + post treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d400fd1-c55b-4cef-b8dc-8c88b50e7c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lincs_joined_df = (107404, 1842)\n",
      "kaggle_joined_df = (602, 1841)\n",
      "test_joined_df = (255, 921)\n"
     ]
    }
   ],
   "source": [
    "lincs_joined_df = pd.read_parquet(\"data/lincs_pretreatment.parquet\")\n",
    "kaggle_joined_df = pd.read_parquet(\"data/kaggle_pretreatment.parquet\")\n",
    "test_joined_df = pd.read_parquet(\"data/test_pretreatment.parquet\")\n",
    "print(f\"lincs_joined_df = {lincs_joined_df.shape}\\nkaggle_joined_df = {kaggle_joined_df.shape}\\ntest_joined_df = {test_joined_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96590245-ca33-4fe3-8092-848b2286fb08",
   "metadata": {},
   "source": [
    "#### Kaggle provided data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ec827ed-5c57-40df-ba5b-e2219b2e1e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de_train = (614, 18216)\n",
      "id_map = (255, 2)\n"
     ]
    }
   ],
   "source": [
    "de_train = pd.read_parquet('data/de_train.parquet')\n",
    "id_map = pd.read_csv('data/id_map.csv',index_col='id')\n",
    "print(f\"de_train = {de_train.shape}\\nid_map = {id_map.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c526082f-43a6-4ac2-b099-16f9426d067c",
   "metadata": {},
   "source": [
    "#### Define features of interest and sort data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "588df052-af79-4dc8-9349-93a02e923f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transcriptome_cols = (18211,)\n",
      "landmark_cols = (918,)\n"
     ]
    }
   ],
   "source": [
    "features = ['cell_type', 'sm_name']\n",
    "multiindex_features = [(\"label\",'cell_type'),(\"label\",'sm_name')]\n",
    "\n",
    "transcriptome_cols = de_train.columns[5:]\n",
    "landmark_cols = kaggle_joined_df[\"post_treatment\"].columns\n",
    "print(f\"transcriptome_cols = {transcriptome_cols.shape}\\nlandmark_cols = {landmark_cols.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3cab5c9-b11b-44b3-a4d2-73d4b8a57bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique molecules = 1896.\n",
      "Number of unique cell types = 36.\n"
     ]
    }
   ],
   "source": [
    "unique_sm_name = pd.concat([lincs_joined_df[(\"label\",\"sm_name\")],kaggle_joined_df[(\"label\",\"sm_name\")]]).drop_duplicates().reset_index(drop=True)\n",
    "unique_cell_type = pd.concat([lincs_joined_df[(\"label\",\"cell_type\")],kaggle_joined_df[(\"label\",\"cell_type\")]]).drop_duplicates().reset_index(drop=True)\n",
    "print(f\"Number of unique molecules = {len(unique_sm_name)}.\\nNumber of unique cell types = {len(unique_cell_type)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe4c7429-9062-45a5-8c88-7b8c4eda8204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We only need to sort these two dataframes because they represent the same underlying dataset.\n",
    "de_train = de_train.query(\"~control\").sort_values(features)\n",
    "kaggle_joined_df = kaggle_joined_df.sort_values(multiindex_features)\n",
    "# Sanity check that these dfs align.\n",
    "genes_align = (kaggle_joined_df[\"post_treatment\"] == de_train[landmark_cols]).all(axis=None)\n",
    "labels_align = (kaggle_joined_df[\"label\"][features] == de_train[features]).all(axis=None)\n",
    "genes_align and labels_align"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa9dc95-c087-4b47-a1f4-1798d594a3b1",
   "metadata": {},
   "source": [
    "#### CV splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbcbabde-a608-441c-b942-67cf9f33aefe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_cells_only_df = kaggle_joined_df[kaggle_joined_df[\"label\"][\"cell_type\"].isin([\"B cells\", \"Myeloid cells\"])][multiindex_features]\n",
    "len(eval_cells_only_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40d753c9-b552-4c04-a63e-4cbb4504ec34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold = 0 of shape (10, 2)\n",
      "fold = 1 of shape (10, 2)\n",
      "fold = 2 of shape (10, 2)\n"
     ]
    }
   ],
   "source": [
    "fold_to_eval_df = {}\n",
    "skf = KFold(n_splits=3, random_state=42, shuffle=True)\n",
    "for i,(t,v) in enumerate(skf.split(eval_cells_only_df)):\n",
    "    fold_to_eval_df[i] = eval_cells_only_df.iloc[v]\n",
    "\n",
    "for i, df in fold_to_eval_df.items():\n",
    "    print(f\"fold = {i} of shape {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "618b44f7-71a6-48e2-a2dd-0d396af97d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fold 0 as validation set:\n",
      "Train data = (107994, 1843)\n",
      "Validation data = (12, 1841)\n"
     ]
    }
   ],
   "source": [
    "def make_mask(fold):\n",
    "    val = fold_to_eval_df[fold]\n",
    "    return kaggle_joined_df[(\"label\",\"sm_name\")].isin(val[(\"label\",\"sm_name\")]) & kaggle_joined_df[(\"label\",\"cell_type\")].isin(val[(\"label\",\"cell_type\")])\n",
    "\n",
    "print(\"Using fold 0 as validation set:\")\n",
    "print(f\"Train data = {pd.concat([kaggle_joined_df[~make_mask(0)],lincs_joined_df]).shape}\")\n",
    "print(f\"Validation data = {kaggle_joined_df[make_mask(0)].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c35d4cde-5324-4c0b-81a2-975c25d843ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(torch.nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(Translator,self).__init__()\n",
    "        # This will eventually be changed to a GNN\n",
    "        self.smiles_embed = torch.nn.Embedding(len(unique_sm_name), config[\"sm_emb_size\"])\n",
    "\n",
    "        # This needs to be able to handle out of dictionary\n",
    "        self.cell_embed = torch.nn.Embedding(len(unique_cell_type), config[\"cell_emb_size\"])\n",
    "\n",
    "        self.config = config\n",
    "        input_dim = config[\"sm_emb_size\"] + config[\"cell_emb_size\"] + config[\"latent_dim\"]\n",
    "        self.translation = utils.make_sequential(input_dim,config[\"hidden_dim\"],config[\"latent_dim\"],config[\"dropout\"])\n",
    "\n",
    "    def forward(self,inp,z):\n",
    "        sm_emb = self.smiles_embed(inp[\"sm_name\"])\n",
    "        ct_emb = self.cell_embed(inp[\"cell_type\"])\n",
    "        x = torch.cat((sm_emb, ct_emb, z), dim=1)\n",
    "        return self.translation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40c3d8cf-fbde-4aeb-905d-77d647bb7621",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNVAE(torch.nn.Module):\n",
    "    cell_type_map = {v: k for k,v in unique_cell_type.to_dict().items()}\n",
    "    sm_name_map = {v: k for k,v in unique_sm_name.to_dict().items()}\n",
    "    \n",
    "    def __init__(self,config):\n",
    "        super(RNVAE,self).__init__()\n",
    "        self.vae = autoencoder.AutoEncoder(target_dim=len(landmark_cols),config=config)\n",
    "        self.translator = Translator(config)\n",
    "\n",
    "    # This is super slow because we are iterating.\n",
    "    @classmethod\n",
    "    def make_input(cls, df, disabletqdm=True):\n",
    "        ct = df[(\"label\",\"cell_type\")].map(cls.cell_type_map)\n",
    "        sm = df[(\"label\",\"sm_name\")].map(cls.sm_name_map)\n",
    "        return [{\"cell_type\":torch.tensor(ct.iloc[i]),\n",
    "                \"sm_name\":torch.tensor(sm.iloc[i]),\n",
    "                \"pre_treatment\":torch.tensor(df[\"pre_treatment\"].iloc[i].to_numpy(),dtype=torch.float),\n",
    "                \"post_treatment\":torch.tensor(df[\"post_treatment\"].iloc[i].to_numpy(),dtype=torch.float)} for i in tqdm.tqdm(range(len(df)),disable=disabletqdm)]\n",
    "\n",
    "    @classmethod\n",
    "    def make_input_new(cls, df):\n",
    "        ct = torch.tensor(df[(\"label\",\"cell_type\")].map(cls.cell_type_map).to_numpy())\n",
    "        sm = torch.tensor(df[(\"label\",\"sm_name\")].map(cls.sm_name_map).to_numpy())\n",
    "        pre = torch.tensor(df[\"pre_treatment\"].to_numpy(),dtype=torch.float32)\n",
    "        post = torch.tensor(df[\"post_treatment\"].to_numpy(),dtype=torch.float32)\n",
    "        \n",
    "        return [{\"cell_type\":ct[i],\n",
    "                \"sm_name\":sm[i],\n",
    "                \"pre_treatment\":pre[i],\n",
    "                \"post_treatment\":post[i]} for i in range(len(df))]\n",
    "\n",
    "    @classmethod\n",
    "    def make_test(cls,df):\n",
    "        ct = torch.tensor(df[(\"label\",\"cell_type\")].map(cls.cell_type_map).to_numpy())\n",
    "        sm = torch.tensor(df[(\"label\",\"sm_name\")].map(cls.sm_name_map).to_numpy())\n",
    "        pre = torch.tensor(df[\"pre_treatment\"].to_numpy(),dtype=torch.float32)\n",
    "        \n",
    "        return [{\"cell_type\":ct[i],\n",
    "                \"sm_name\":sm[i],\n",
    "                \"pre_treatment\":pre[i]} for i in range(len(df))]\n",
    "    \n",
    "    def forward(self,inp):\n",
    "        latent = self.vae.latent(inp[\"pre_treatment\"])\n",
    "        z_prime = self.translator(inp,latent[\"z\"])\n",
    "        x_hat = self.vae.decode(z_prime)\n",
    "        return {\"x_hat\":x_hat, \"mu\": latent[\"mu\"], \"log_var\":latent[\"log_var\"]}\n",
    "\n",
    "    def loss_function(self,fwd,inp):\n",
    "        return self.vae.loss_function(fwd,inp[\"post_treatment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0a1930d-433a-47d1-82b7-56637dbbf4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Imputer(torch.nn.Module):\n",
    "    def __init__(self,config,rnvae):\n",
    "        super(Imputer,self).__init__()\n",
    "        self.impute_loss_weight = config[\"impute_loss_weight\"]\n",
    "        self.imp = utils.make_sequential(len(landmark_cols),config[\"hidden_dim\"],len(transcriptome_cols),config[\"dropout\"])\n",
    "        self.rnvae = rnvae\n",
    "\n",
    "    @classmethod\n",
    "    def make_input(cls, mask):\n",
    "        kg_df = kaggle_joined_df[mask]\n",
    "        trn_df = de_train[mask]\n",
    "        rninp = RNVAE.make_input(kg_df)\n",
    "        trm = trn_df[transcriptome_cols]\n",
    "        for i,inp in enumerate(rninp):\n",
    "            inp[\"transcriptome\"] = torch.tensor(trm.iloc[i].to_numpy(), dtype=torch.float)\n",
    "        return rninp\n",
    "\n",
    "    def forward(self,inp):\n",
    "        fwd = self.rnvae(inp)\n",
    "        trm = self.imp(fwd[\"x_hat\"])\n",
    "        fwd[\"transcriptome\"] = trm\n",
    "        return fwd\n",
    "\n",
    "    def loss_function(self,fwd,inp):\n",
    "        trm_loss = torch.nn.functional.mse_loss(fwd[\"transcriptome\"], inp[\"transcriptome\"])\n",
    "        lossdict = self.rnvae.loss_function(fwd,inp)\n",
    "        lossdict[\"loss\"] += self.impute_loss_weight*trm_loss\n",
    "        lossdict[\"Transcriptome_Loss\"] = trm_loss.detach()\n",
    "        return lossdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292fd1f0-aea5-4f60-b943-4376884cc608",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▏                                   | 566/10000 [00:02<00:54, 173.26it/s]"
     ]
    }
   ],
   "source": [
    "bsz = 512\n",
    "lincs_sample = lincs_joined_df.sample(10000)\n",
    "rnvae_inp_new = RNVAE.make_input_new(lincs_sample)\n",
    "rnvae_inp = RNVAE.make_input(lincs_sample,disabletqdm=False)\n",
    "print(len(rnvae_inp_new[0][\"pre_treatment\"]),len(rnvae_inp[0][\"pre_treatment\"]))\n",
    "print((rnvae_inp_new[0][\"pre_treatment\"] == rnvae_inp[0][\"pre_treatment\"]).all())\n",
    "\n",
    "for k, v in rnvae_inp_new[0].items():\n",
    "    print(k,v.dtype,v.shape)\n",
    "\n",
    "for k, v in rnvae_inp[0].items():\n",
    "    print(k,v.dtype,v.shape)\n",
    "\n",
    "rnvae_loader = torch.utils.data.DataLoader(rnvae_inp, batch_size=bsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fed2d2f9-716b-44e3-8379-95ea3ecdaa92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell_type torch.int64\n",
      "sm_name torch.int64\n",
      "pre_treatment torch.float32\n",
      "post_treatment torch.float32\n",
      "transcriptome torch.float32\n",
      "\n",
      "cell_type torch.int64\n",
      "sm_name torch.int64\n",
      "pre_treatment torch.float32\n",
      "post_treatment torch.float32\n",
      "transcriptome torch.float32\n",
      "cell_type torch.int64\n",
      "sm_name torch.int64\n",
      "pre_treatment torch.float32\n",
      "post_treatment torch.float32\n",
      "transcriptome torch.float32\n",
      "\n",
      "cell_type torch.int64\n",
      "sm_name torch.int64\n",
      "pre_treatment torch.float32\n",
      "post_treatment torch.float32\n",
      "transcriptome torch.float32\n",
      "cell_type torch.int64\n",
      "sm_name torch.int64\n",
      "pre_treatment torch.float32\n",
      "post_treatment torch.float32\n",
      "transcriptome torch.float32\n",
      "\n",
      "cell_type torch.int64\n",
      "sm_name torch.int64\n",
      "pre_treatment torch.float32\n",
      "post_treatment torch.float32\n",
      "transcriptome torch.float32\n"
     ]
    }
   ],
   "source": [
    "train_loaders = []\n",
    "eval_loaders = []\n",
    "for fold in fold_to_eval_df:\n",
    "    traind = Imputer.make_input(~make_mask(fold))\n",
    "    for k, v in traind[0].items():\n",
    "        print(k,v.dtype)\n",
    "    print()\n",
    "    train_loaders.append(torch.utils.data.DataLoader(traind, batch_size=bsz))\n",
    "    \n",
    "    evald = Imputer.make_input(make_mask(fold))\n",
    "    for k, v in evald[0].items():\n",
    "        print(k,v.dtype)\n",
    "    eval_loaders.append(torch.utils.data.DataLoader(evald, batch_size=len(evald)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "81e19f66-3b3d-40fd-8733-0c6160b845b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch(models):\n",
    "    def _epoch(model,opt,loader):\n",
    "        for batch in loader:\n",
    "            opt.zero_grad()\n",
    "            fwd = model(batch)\n",
    "            loss = model.loss_function(fwd,batch)[\"loss\"]\n",
    "            if torch.isnan(loss):\n",
    "                return loss.detach()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        \n",
    "        return loss.detach()\n",
    "\n",
    "\n",
    "    loss = _epoch(models[\"rnvae\"],models[\"rnvae_opt\"],models[\"rnvae_loader\"])\n",
    "    if torch.isnan(loss):\n",
    "        return loss\n",
    "\n",
    "    loss = _epoch(models[\"imputer\"],models[\"impute_opt\"],models[\"train_loader\"])\n",
    "    if torch.isnan(loss):\n",
    "        return loss\n",
    "\n",
    "    imputer = models[\"imputer\"]\n",
    "    with torch.no_grad():\n",
    "        eval = next(iter(models[\"eval_loader\"]))\n",
    "        fwd = imputer(eval)\n",
    "        # The eval loss we wish to optimize is how well the model\n",
    "        # predicts the full transcriptome.\n",
    "        return imputer.loss_function(fwd,eval)[\"Transcriptome_Loss\"]\n",
    "\n",
    "def make_models(config, input_data, fold):\n",
    "    rnvae = RNVAE(config)\n",
    "    imputer = Imputer(config,rnvae)\n",
    "    return {\n",
    "        \"rnvae\": rnvae,\n",
    "        \"imputer\": imputer,\n",
    "        \"rnvae_opt\": torch.optim.Adam(rnvae.parameters(), lr=config[\"lr_rnvae\"]),\n",
    "        \"impute_opt\": torch.optim.Adam(imputer.parameters(), lr=config[\"lr_imputer\"]),\n",
    "        \"rnvae_loader\": input_data[\"rnvae_loader\"], # There is just one rnvae_loader shared across all folds\n",
    "        \"train_loader\": input_data[\"train_loaders\"][fold],\n",
    "        \"eval_loader\": input_data[\"eval_loaders\"][fold]\n",
    "    }\n",
    "    \n",
    "def train_model(config, input_data):    \n",
    "    def report(epoch,result):\n",
    "        train.report(result)\n",
    "        # if epoch % 10 == 0:\n",
    "        #     print(epoch,result)\n",
    "    \n",
    "    all_models = []\n",
    "    for fold in input_data[\"fold_to_eval_df\"]:\n",
    "        all_models.append(make_models(config, input_data, fold))\n",
    "\n",
    "    for i in range(input_data[\"epochs\"]):\n",
    "        losses = []\n",
    "        for fold in input_data[\"fold_to_eval_df\"]:\n",
    "            losses.append(epoch(all_models[fold]))\n",
    "        \n",
    "        if np.any(np.isnan(losses)):\n",
    "            report(i,{input_data[\"metric\"]: np.nan, \"done\": True})\n",
    "        else:\n",
    "            report(i,{input_data[\"metric\"]: np.mean(losses)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6172cda-8d6e-4e80-8f1b-4908c94ec968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-10-21 15:26:35</td></tr>\n",
       "<tr><td>Running for: </td><td>00:02:45.27        </td></tr>\n",
       "<tr><td>Memory:      </td><td>4.7/8.0 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=14<br>Bracket: Iter 20.000: -18.148491859436035 | Iter 5.000: -19.202662467956543<br>Logical resource usage: 1.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name          </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  cell_emb_size</th><th style=\"text-align: right;\">  dropout</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">  impute_loss_weight</th><th style=\"text-align: right;\">  kld_weight</th><th style=\"text-align: right;\">  latent_dim</th><th style=\"text-align: right;\">  lr_imputer</th><th style=\"text-align: right;\">   lr_rnvae</th><th style=\"text-align: right;\">  sm_emb_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">          mse</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_96b7866f</td><td>TERMINATED</td><td>127.0.0.1:27725</td><td style=\"text-align: right;\">             14</td><td style=\"text-align: right;\">0.325083 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">            0.756603</td><td style=\"text-align: right;\">    3.0455  </td><td style=\"text-align: right;\">         288</td><td style=\"text-align: right;\"> 4.97523e-05</td><td style=\"text-align: right;\">0.0258026  </td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         2.57376</td><td style=\"text-align: right;\">nan          </td></tr>\n",
       "<tr><td>train_model_37a0f3c7</td><td>TERMINATED</td><td>127.0.0.1:27725</td><td style=\"text-align: right;\">              7</td><td style=\"text-align: right;\">0.0481424</td><td style=\"text-align: right;\">         468</td><td style=\"text-align: right;\">            0.142023</td><td style=\"text-align: right;\">    0.224891</td><td style=\"text-align: right;\">          47</td><td style=\"text-align: right;\"> 0.150954   </td><td style=\"text-align: right;\">6.51287e-05</td><td style=\"text-align: right;\">           17</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.04511</td><td style=\"text-align: right;\">nan          </td></tr>\n",
       "<tr><td>train_model_b29663eb</td><td>TERMINATED</td><td>127.0.0.1:27725</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">0.0846026</td><td style=\"text-align: right;\">         466</td><td style=\"text-align: right;\">            0.546392</td><td style=\"text-align: right;\">    0.176804</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\"> 0.00014273 </td><td style=\"text-align: right;\">0.00155269 </td><td style=\"text-align: right;\">            6</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">       145.703  </td><td style=\"text-align: right;\"> 17.7377     </td></tr>\n",
       "<tr><td>train_model_3442c86b</td><td>TERMINATED</td><td>127.0.0.1:27747</td><td style=\"text-align: right;\">              4</td><td style=\"text-align: right;\">0.952439 </td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">            3.62338 </td><td style=\"text-align: right;\">    2.20191 </td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\"> 0.00897108 </td><td style=\"text-align: right;\">0.000823534</td><td style=\"text-align: right;\">            9</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.40722</td><td style=\"text-align: right;\">nan          </td></tr>\n",
       "<tr><td>train_model_b39c8b7d</td><td>TERMINATED</td><td>127.0.0.1:27747</td><td style=\"text-align: right;\">             16</td><td style=\"text-align: right;\">0.317211 </td><td style=\"text-align: right;\">         226</td><td style=\"text-align: right;\">            0.521017</td><td style=\"text-align: right;\">    6.50339 </td><td style=\"text-align: right;\">          40</td><td style=\"text-align: right;\"> 0.0315213  </td><td style=\"text-align: right;\">0.00651967 </td><td style=\"text-align: right;\">            5</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        12.2709 </td><td style=\"text-align: right;\">  1.39597e+06</td></tr>\n",
       "<tr><td>train_model_7d962c2d</td><td>TERMINATED</td><td>127.0.0.1:27758</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">0.321315 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">            4.68073 </td><td style=\"text-align: right;\">    6.58762 </td><td style=\"text-align: right;\">         305</td><td style=\"text-align: right;\"> 0.000691152</td><td style=\"text-align: right;\">0.115826   </td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.23891</td><td style=\"text-align: right;\">nan          </td></tr>\n",
       "<tr><td>train_model_6b6dd4c5</td><td>TERMINATED</td><td>127.0.0.1:27758</td><td style=\"text-align: right;\">              8</td><td style=\"text-align: right;\">0.934079 </td><td style=\"text-align: right;\">          11</td><td style=\"text-align: right;\">            3.31605 </td><td style=\"text-align: right;\">    0.543498</td><td style=\"text-align: right;\">          12</td><td style=\"text-align: right;\"> 0.00295545 </td><td style=\"text-align: right;\">0.162567   </td><td style=\"text-align: right;\">           10</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.24743</td><td style=\"text-align: right;\">nan          </td></tr>\n",
       "<tr><td>train_model_6cfd59f2</td><td>TERMINATED</td><td>127.0.0.1:27758</td><td style=\"text-align: right;\">              8</td><td style=\"text-align: right;\">0.276024 </td><td style=\"text-align: right;\">          31</td><td style=\"text-align: right;\">            0.197947</td><td style=\"text-align: right;\">    0.65754 </td><td style=\"text-align: right;\">          12</td><td style=\"text-align: right;\"> 0.0020115  </td><td style=\"text-align: right;\">7.31638e-05</td><td style=\"text-align: right;\">           10</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">        29.1612 </td><td style=\"text-align: right;\"> 18.1587     </td></tr>\n",
       "<tr><td>train_model_2e128526</td><td>TERMINATED</td><td>127.0.0.1:27747</td><td style=\"text-align: right;\">             10</td><td style=\"text-align: right;\">0.384028 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">            0.51023 </td><td style=\"text-align: right;\">    1.3125  </td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\"> 9.98266e-05</td><td style=\"text-align: right;\">0.0818821  </td><td style=\"text-align: right;\">            8</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         3.55792</td><td style=\"text-align: right;\"> 19.523      </td></tr>\n",
       "<tr><td>train_model_c08ef518</td><td>TERMINATED</td><td>127.0.0.1:27747</td><td style=\"text-align: right;\">             17</td><td style=\"text-align: right;\">0.675803 </td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">            6.2698  </td><td style=\"text-align: right;\">    1.41903 </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\"> 0.000103325</td><td style=\"text-align: right;\">0.00579406 </td><td style=\"text-align: right;\">           15</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         5.12363</td><td style=\"text-align: right;\"> 19.3459     </td></tr>\n",
       "<tr><td>train_model_863b8e19</td><td>TERMINATED</td><td>127.0.0.1:27747</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">0.514663 </td><td style=\"text-align: right;\">          48</td><td style=\"text-align: right;\">            0.5583  </td><td style=\"text-align: right;\">    6.02787 </td><td style=\"text-align: right;\">          17</td><td style=\"text-align: right;\"> 6.98663e-05</td><td style=\"text-align: right;\">0.00261553 </td><td style=\"text-align: right;\">            8</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         5.14671</td><td style=\"text-align: right;\"> 19.1973     </td></tr>\n",
       "<tr><td>train_model_b19adc5b</td><td>TERMINATED</td><td>127.0.0.1:27747</td><td style=\"text-align: right;\">             18</td><td style=\"text-align: right;\">0.670827 </td><td style=\"text-align: right;\">         111</td><td style=\"text-align: right;\">            5.4313  </td><td style=\"text-align: right;\">    6.19666 </td><td style=\"text-align: right;\">         114</td><td style=\"text-align: right;\"> 0.00775554 </td><td style=\"text-align: right;\">0.0529458  </td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.53185</td><td style=\"text-align: right;\">nan          </td></tr>\n",
       "<tr><td>train_model_43682e89</td><td>TERMINATED</td><td>127.0.0.1:27747</td><td style=\"text-align: right;\">             15</td><td style=\"text-align: right;\">0.559119 </td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">            0.241502</td><td style=\"text-align: right;\">    4.12461 </td><td style=\"text-align: right;\">          20</td><td style=\"text-align: right;\"> 0.000949058</td><td style=\"text-align: right;\">0.1275     </td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.82438</td><td style=\"text-align: right;\">nan          </td></tr>\n",
       "<tr><td>train_model_b04d1326</td><td>TERMINATED</td><td>127.0.0.1:27747</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">0.222105 </td><td style=\"text-align: right;\">         853</td><td style=\"text-align: right;\">            1.61243 </td><td style=\"text-align: right;\">    3.96382 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\"> 0.110544   </td><td style=\"text-align: right;\">0.081503   </td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        95.7429 </td><td style=\"text-align: right;\">  1.01662e+10</td></tr>\n",
       "<tr><td>train_model_843d534b</td><td>TERMINATED</td><td>127.0.0.1:27782</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">0.299425 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">            1.80232 </td><td style=\"text-align: right;\">    0.151311</td><td style=\"text-align: right;\">         778</td><td style=\"text-align: right;\"> 0.00245879 </td><td style=\"text-align: right;\">0.000349084</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        22.4612 </td><td style=\"text-align: right;\"> 19.476      </td></tr>\n",
       "<tr><td>train_model_45fe8471</td><td>TERMINATED</td><td>127.0.0.1:27758</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">0.785634 </td><td style=\"text-align: right;\">          15</td><td style=\"text-align: right;\">            0.757505</td><td style=\"text-align: right;\">    1.67702 </td><td style=\"text-align: right;\">         378</td><td style=\"text-align: right;\"> 0.000628513</td><td style=\"text-align: right;\">0.109961   </td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.14975</td><td style=\"text-align: right;\">nan          </td></tr>\n",
       "<tr><td>train_model_7984eb9f</td><td>TERMINATED</td><td>127.0.0.1:27758</td><td style=\"text-align: right;\">             12</td><td style=\"text-align: right;\">0.713306 </td><td style=\"text-align: right;\">           7</td><td style=\"text-align: right;\">            3.10464 </td><td style=\"text-align: right;\">    0.744008</td><td style=\"text-align: right;\">           6</td><td style=\"text-align: right;\"> 0.000867202</td><td style=\"text-align: right;\">0.0181453  </td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        18.186  </td><td style=\"text-align: right;\"> 19.2227     </td></tr>\n",
       "<tr><td>train_model_eb8fd361</td><td>TERMINATED</td><td>127.0.0.1:27782</td><td style=\"text-align: right;\">             20</td><td style=\"text-align: right;\">0.437422 </td><td style=\"text-align: right;\">          19</td><td style=\"text-align: right;\">            0.901119</td><td style=\"text-align: right;\">    1.02677 </td><td style=\"text-align: right;\">         735</td><td style=\"text-align: right;\"> 0.0431149  </td><td style=\"text-align: right;\">0.000254157</td><td style=\"text-align: right;\">           12</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.88852</td><td style=\"text-align: right;\">nan          </td></tr>\n",
       "<tr><td>train_model_451a7049</td><td>TERMINATED</td><td>127.0.0.1:27782</td><td style=\"text-align: right;\">              8</td><td style=\"text-align: right;\">0.909988 </td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">            6.81248 </td><td style=\"text-align: right;\">    0.216594</td><td style=\"text-align: right;\">          74</td><td style=\"text-align: right;\"> 0.0454463  </td><td style=\"text-align: right;\">5.79958e-05</td><td style=\"text-align: right;\">           18</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.91013</td><td style=\"text-align: right;\">nan          </td></tr>\n",
       "<tr><td>train_model_7f939032</td><td>TERMINATED</td><td>127.0.0.1:27758</td><td style=\"text-align: right;\">              4</td><td style=\"text-align: right;\">0.570783 </td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">            2.2387  </td><td style=\"text-align: right;\">    0.434755</td><td style=\"text-align: right;\">          53</td><td style=\"text-align: right;\"> 0.000221108</td><td style=\"text-align: right;\">0.000348455</td><td style=\"text-align: right;\">            8</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         3.83947</td><td style=\"text-align: right;\"> 19.3475     </td></tr>\n",
       "<tr><td>train_model_0aa10a1e</td><td>TERMINATED</td><td>127.0.0.1:27782</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">0.716842 </td><td style=\"text-align: right;\">          25</td><td style=\"text-align: right;\">            2.15536 </td><td style=\"text-align: right;\">    0.147293</td><td style=\"text-align: right;\">         159</td><td style=\"text-align: right;\"> 9.79262e-05</td><td style=\"text-align: right;\">0.000147928</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         8.49062</td><td style=\"text-align: right;\"> 19.208      </td></tr>\n",
       "<tr><td>train_model_82eab9bd</td><td>TERMINATED</td><td>127.0.0.1:27758</td><td style=\"text-align: right;\">             13</td><td style=\"text-align: right;\">0.0758295</td><td style=\"text-align: right;\">          22</td><td style=\"text-align: right;\">            0.480457</td><td style=\"text-align: right;\">    6.45309 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\"> 0.000509099</td><td style=\"text-align: right;\">0.0323129  </td><td style=\"text-align: right;\">            6</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        22.8418 </td><td style=\"text-align: right;\"> 19.1285     </td></tr>\n",
       "<tr><td>train_model_0f1b0bfd</td><td>TERMINATED</td><td>127.0.0.1:27782</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">0.286195 </td><td style=\"text-align: right;\">           9</td><td style=\"text-align: right;\">            3.59043 </td><td style=\"text-align: right;\">    1.20858 </td><td style=\"text-align: right;\">         169</td><td style=\"text-align: right;\"> 0.0379751  </td><td style=\"text-align: right;\">0.000153994</td><td style=\"text-align: right;\">           10</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.9487 </td><td style=\"text-align: right;\">nan          </td></tr>\n",
       "<tr><td>train_model_45faa05a</td><td>TERMINATED</td><td>127.0.0.1:27782</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">0.957514 </td><td style=\"text-align: right;\">          75</td><td style=\"text-align: right;\">            0.188961</td><td style=\"text-align: right;\">    0.990438</td><td style=\"text-align: right;\">         969</td><td style=\"text-align: right;\"> 9.72448e-05</td><td style=\"text-align: right;\">0.000541347</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        25.4072 </td><td style=\"text-align: right;\">379.888      </td></tr>\n",
       "<tr><td>train_model_ef30622f</td><td>TERMINATED</td><td>127.0.0.1:27758</td><td style=\"text-align: right;\">              8</td><td style=\"text-align: right;\">0.896092 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">            6.51121 </td><td style=\"text-align: right;\">    5.29446 </td><td style=\"text-align: right;\">          12</td><td style=\"text-align: right;\"> 0.000355641</td><td style=\"text-align: right;\">0.00796948 </td><td style=\"text-align: right;\">           18</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         3.28853</td><td style=\"text-align: right;\"> 19.524      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-21 15:23:51,280\tWARNING worker.py:2058 -- Warning: The actor ImplicitFunc is very large (92 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n",
      "2023-10-21 15:26:35,748\tINFO tune.py:1143 -- Total run time: 165.48 seconds (165.27 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/laurasisson/ray_results/train_model_2023-10-21_15-23-50/train_model_b29663eb_3_cell_emb_size=1,dropout=0.0846,hidden_dim=466,impute_loss_weight=0.5464,kld_weight=0.1768,latent_dim=4,lr_i_2023-10-21_15-23-59\n",
      "CONFIG: {'cell_emb_size': 1, 'dropout': 0.08460261005467273, 'hidden_dim': 466, 'impute_loss_weight': 0.5463921138401946, 'kld_weight': 0.17680367176040052, 'latent_dim': 4, 'lr_imputer': 0.00014272996901395086, 'lr_rnvae': 0.001552694731400991, 'sm_emb_size': 6}\n",
      "METRICS: {'mse': 17.737665, 'timestamp': 1697916395, 'done': True, 'training_iteration': 25, 'trial_id': 'b29663eb', 'date': '2023-10-21_15-26-35', 'time_this_iter_s': 1.8166158199310303, 'time_total_s': 145.70338678359985, 'pid': 27725, 'hostname': 'Lauras-Air', 'node_ip': '127.0.0.1', 'config': {'cell_emb_size': 1, 'dropout': 0.08460261005467273, 'hidden_dim': 466, 'impute_loss_weight': 0.5463921138401946, 'kld_weight': 0.17680367176040052, 'latent_dim': 4, 'lr_imputer': 0.00014272996901395086, 'lr_rnvae': 0.001552694731400991, 'sm_emb_size': 6}, 'time_since_restore': 145.70338678359985, 'iterations_since_restore': 25, 'checkpoint_dir_name': None, 'experiment_tag': '3_cell_emb_size=1,dropout=0.0846,hidden_dim=466,impute_loss_weight=0.5464,kld_weight=0.1768,latent_dim=4,lr_imputer=0.0001,lr_rnvae=0.0016,sm_emb_size=6'}\n"
     ]
    }
   ],
   "source": [
    "num_samples = 25\n",
    "epochs = 25\n",
    "metric = \"mse\"\n",
    "\n",
    "input_data = {\n",
    "    \"rnvae_loader\": rnvae_loader,\n",
    "    \"train_loaders\": train_loaders,\n",
    "    \"eval_loaders\": eval_loaders,\n",
    "    \"fold_to_eval_df\": fold_to_eval_df,\n",
    "    \"epochs\": epochs,\n",
    "    \"metric\": metric\n",
    "}\n",
    "\n",
    "example_config = {\n",
    "    \"lr_rnvae\": 1e-3,\n",
    "    \"lr_imputer\": 1e-4,\n",
    "    \"dropout\": .1,\n",
    "    \"sm_emb_size\": 64,\n",
    "    \"cell_emb_size\": 32,\n",
    "    \"latent_dim\": 256,\n",
    "    \"hidden_dim\": 512,\n",
    "    \"kld_weight\": 1,\n",
    "    \"impute_loss_weight\": 2,\n",
    "}\n",
    "\n",
    "space = {\n",
    "    \"lr_rnvae\": hp.loguniform(\"lr_rnvae\", -10, -1),\n",
    "    \"lr_imputer\": hp.loguniform(\"lr_imputer\", -10, -1),\n",
    "    \"dropout\": hp.uniform(\"dropout\", 0, 1),\n",
    "    \"sm_emb_size\": scope.int(hp.qloguniform(\"sm_emb_size\", 0, 3, 1)),\n",
    "    \"cell_emb_size\": scope.int(hp.qloguniform(\"cell_emb_size\", 0, 3, 1)),\n",
    "    \"latent_dim\": scope.int(hp.qloguniform(\"latent_dim\", 0, 7, 1)),\n",
    "    \"hidden_dim\": scope.int(hp.qloguniform(\"hidden_dim\", 0, 7, 1)),\n",
    "    \"kld_weight\": hp.loguniform(\"kld_weight\", -2, 2),\n",
    "    \"impute_loss_weight\": hp.loguniform(\"impute_loss_weight\", -2, 2),\n",
    "}\n",
    "\n",
    "train_model(example_config,input_data)\n",
    "mode = \"min\"\n",
    "hyperopt_search = HyperOptSearch(space, metric=\"mse\", mode=mode)\n",
    "scheduler = ASHAScheduler(metric=\"mse\", grace_period=5, mode=mode, max_t=epochs)\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_parameters(train_model, input_data=input_data),\n",
    "    tune_config=tune.TuneConfig(\n",
    "        num_samples=num_samples,\n",
    "        search_alg=hyperopt_search,\n",
    "        scheduler=scheduler\n",
    "    ),\n",
    "    run_config=train.RunConfig(\n",
    "        failure_config=train.FailureConfig(fail_fast=False))\n",
    ")\n",
    "results = tuner.fit()\n",
    "\n",
    "best_result = results.get_best_result(metric, mode=mode)\n",
    "print(best_result.path)\n",
    "print(\"CONFIG:\", best_result.config)\n",
    "print(\"METRICS:\", best_result.metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
