{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f279ce28-8ea8-4ccd-9c63-b095228e175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autoencoder\n",
    "import utils\n",
    "import mrrmse\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import random\n",
    "\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from ray import train, tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.hyperopt import HyperOptSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771a0f9e-ed59-4568-b6e8-4599c153eb0d",
   "metadata": {},
   "source": [
    "## Prepare data:\n",
    "#### Read joined data (pre + post treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d400fd1-c55b-4cef-b8dc-8c88b50e7c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lincs_joined_df = (107404, 1842)\n",
      "kaggle_joined_df = (602, 1841)\n",
      "test_joined_df = (255, 921)\n"
     ]
    }
   ],
   "source": [
    "lincs_joined_df = pd.read_parquet(\"data/lincs_pretreatment.parquet\")\n",
    "kaggle_joined_df = pd.read_parquet(\"data/kaggle_pretreatment.parquet\")\n",
    "test_joined_df = pd.read_parquet(\"data/test_pretreatment.parquet\")\n",
    "print(f\"lincs_joined_df = {lincs_joined_df.shape}\\nkaggle_joined_df = {kaggle_joined_df.shape}\\ntest_joined_df = {test_joined_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96590245-ca33-4fe3-8092-848b2286fb08",
   "metadata": {},
   "source": [
    "#### Kaggle provided data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ec827ed-5c57-40df-ba5b-e2219b2e1e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de_train = (614, 18216)\n",
      "id_map = (255, 2)\n"
     ]
    }
   ],
   "source": [
    "de_train = pd.read_parquet('data/de_train.parquet')\n",
    "id_map = pd.read_csv('data/id_map.csv',index_col='id')\n",
    "print(f\"de_train = {de_train.shape}\\nid_map = {id_map.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c526082f-43a6-4ac2-b099-16f9426d067c",
   "metadata": {},
   "source": [
    "#### Define features of interest and sort data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "588df052-af79-4dc8-9349-93a02e923f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transcriptome_cols = (18211,)\n",
      "landmark_cols = (918,)\n"
     ]
    }
   ],
   "source": [
    "features = ['cell_type', 'sm_name']\n",
    "multiindex_features = [(\"label\",'cell_type'),(\"label\",'sm_name')]\n",
    "\n",
    "transcriptome_cols = de_train.columns[5:]\n",
    "landmark_cols = kaggle_joined_df[\"post_treatment\"].columns\n",
    "print(f\"transcriptome_cols = {transcriptome_cols.shape}\\nlandmark_cols = {landmark_cols.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3cab5c9-b11b-44b3-a4d2-73d4b8a57bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique molecules = 1896.\n",
      "Number of unique cell types = 36.\n"
     ]
    }
   ],
   "source": [
    "unique_sm_name = pd.concat([lincs_joined_df[(\"label\",\"sm_name\")],kaggle_joined_df[(\"label\",\"sm_name\")]]).drop_duplicates().reset_index(drop=True)\n",
    "unique_cell_type = pd.concat([lincs_joined_df[(\"label\",\"cell_type\")],kaggle_joined_df[(\"label\",\"cell_type\")]]).drop_duplicates().reset_index(drop=True)\n",
    "print(f\"Number of unique molecules = {len(unique_sm_name)}.\\nNumber of unique cell types = {len(unique_cell_type)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe4c7429-9062-45a5-8c88-7b8c4eda8204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We only need to sort these two dataframes because they represent the same underlying dataset.\n",
    "de_train = de_train.query(\"~control\").sort_values(features)\n",
    "kaggle_joined_df = kaggle_joined_df.sort_values(multiindex_features)\n",
    "# Sanity check that these dfs align.\n",
    "genes_align = (kaggle_joined_df[\"post_treatment\"] == de_train[landmark_cols]).all(axis=None)\n",
    "labels_align = (kaggle_joined_df[\"label\"][features] == de_train[features]).all(axis=None)\n",
    "genes_align and labels_align"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa9dc95-c087-4b47-a1f4-1798d594a3b1",
   "metadata": {},
   "source": [
    "#### CV splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbcbabde-a608-441c-b942-67cf9f33aefe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_cells_only_df = kaggle_joined_df[kaggle_joined_df[\"label\"][\"cell_type\"].isin([\"B cells\", \"Myeloid cells\"])][multiindex_features]\n",
    "len(eval_cells_only_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40d753c9-b552-4c04-a63e-4cbb4504ec34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold = 0 of shape (10, 2)\n",
      "fold = 1 of shape (10, 2)\n",
      "fold = 2 of shape (10, 2)\n"
     ]
    }
   ],
   "source": [
    "fold_to_eval_df = {}\n",
    "skf = KFold(n_splits=3, random_state=42, shuffle=True)\n",
    "for i,(t,v) in enumerate(skf.split(eval_cells_only_df)):\n",
    "    fold_to_eval_df[i] = eval_cells_only_df.iloc[v]\n",
    "\n",
    "for i, df in fold_to_eval_df.items():\n",
    "    print(f\"fold = {i} of shape {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "618b44f7-71a6-48e2-a2dd-0d396af97d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fold 0 as validation set:\n",
      "Train data = (107994, 1843)\n",
      "Validation data = (12, 1841)\n"
     ]
    }
   ],
   "source": [
    "def make_mask(fold):\n",
    "    val = fold_to_eval_df[fold]\n",
    "    return kaggle_joined_df[(\"label\",\"sm_name\")].isin(val[(\"label\",\"sm_name\")]) & kaggle_joined_df[(\"label\",\"cell_type\")].isin(val[(\"label\",\"cell_type\")])\n",
    "\n",
    "print(\"Using fold 0 as validation set:\")\n",
    "print(f\"Train data = {pd.concat([kaggle_joined_df[~make_mask(0)],lincs_joined_df]).shape}\")\n",
    "print(f\"Validation data = {kaggle_joined_df[make_mask(0)].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c35d4cde-5324-4c0b-81a2-975c25d843ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(torch.nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(Translator,self).__init__()\n",
    "        # This will eventually be changed to a GNN\n",
    "        self.smiles_embed = torch.nn.Embedding(len(unique_sm_name), config[\"sm_emb_size\"])\n",
    "\n",
    "        # This needs to be able to handle out of dictionary\n",
    "        self.cell_embed = torch.nn.Embedding(len(unique_cell_type), config[\"cell_emb_size\"])\n",
    "\n",
    "        self.config = config\n",
    "        input_dim = config[\"sm_emb_size\"] + config[\"cell_emb_size\"] + config[\"latent_dim\"]\n",
    "        self.translation = utils.make_sequential(input_dim,config[\"hidden_dim\"],config[\"latent_dim\"],config[\"dropout\"])\n",
    "\n",
    "    def forward(self,inp,z):\n",
    "        sm_emb = self.smiles_embed(inp[\"sm_name\"])\n",
    "        ct_emb = self.cell_embed(inp[\"cell_type\"])\n",
    "        x = torch.cat((sm_emb, ct_emb, z), dim=1)\n",
    "        return self.translation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40c3d8cf-fbde-4aeb-905d-77d647bb7621",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNVAE(torch.nn.Module):\n",
    "    cell_type_map = {v: k for k,v in unique_cell_type.to_dict().items()}\n",
    "    sm_name_map = {v: k for k,v in unique_sm_name.to_dict().items()}\n",
    "    \n",
    "    def __init__(self,config):\n",
    "        super(RNVAE,self).__init__()\n",
    "        self.vae = autoencoder.AutoEncoder(target_dim=len(landmark_cols),config=config)\n",
    "        self.translator = Translator(config)\n",
    "\n",
    "    # This is super slow because we are iterating.\n",
    "    @classmethod\n",
    "    def make_input(cls, df, disabletqdm=True):\n",
    "        ct = df[(\"label\",\"cell_type\")].map(cls.cell_type_map)\n",
    "        sm = df[(\"label\",\"sm_name\")].map(cls.sm_name_map)\n",
    "        return [{\"cell_type\":torch.tensor(ct.iloc[i]),\n",
    "                \"sm_name\":torch.tensor(sm.iloc[i]),\n",
    "                \"pre_treatment\":torch.tensor(df[\"pre_treatment\"].iloc[i].to_numpy(),dtype=torch.float),\n",
    "                \"post_treatment\":torch.tensor(df[\"post_treatment\"].iloc[i].to_numpy(),dtype=torch.float)} for i in tqdm.tqdm(range(len(df)),disable=disabletqdm)]\n",
    "\n",
    "    @classmethod\n",
    "    def make_input_new(cls, df):\n",
    "        ct = torch.tensor(df[(\"label\",\"cell_type\")].map(cls.cell_type_map).to_numpy())\n",
    "        sm = torch.tensor(df[(\"label\",\"sm_name\")].map(cls.sm_name_map).to_numpy())\n",
    "        pre = torch.tensor(df[\"pre_treatment\"].to_numpy(),dtype=torch.float32)\n",
    "        post = torch.tensor(df[\"post_treatment\"].to_numpy(),dtype=torch.float32)\n",
    "        \n",
    "        return [{\"cell_type\":ct[i],\n",
    "                \"sm_name\":sm[i],\n",
    "                \"pre_treatment\":pre[i],\n",
    "                \"post_treatment\":post[i]} for i in range(len(df))]\n",
    "\n",
    "    @classmethod\n",
    "    def make_test(cls,df):\n",
    "        ct = torch.tensor(df[(\"label\",\"cell_type\")].map(cls.cell_type_map).to_numpy())\n",
    "        sm = torch.tensor(df[(\"label\",\"sm_name\")].map(cls.sm_name_map).to_numpy())\n",
    "        pre = torch.tensor(df[\"pre_treatment\"].to_numpy(),dtype=torch.float32)\n",
    "        \n",
    "        return [{\"cell_type\":ct[i],\n",
    "                \"sm_name\":sm[i],\n",
    "                \"pre_treatment\":pre[i]} for i in range(len(df))]\n",
    "    \n",
    "    def forward(self,inp):\n",
    "        latent = self.vae.latent(inp[\"pre_treatment\"])\n",
    "        z_prime = self.translator(inp,latent[\"z\"])\n",
    "        x_hat = self.vae.decode(z_prime)\n",
    "        return {\"x_hat\":x_hat, \"mu\": latent[\"mu\"], \"log_var\":latent[\"log_var\"]}\n",
    "\n",
    "    def loss_function(self,fwd,inp):\n",
    "        return self.vae.loss_function(fwd,inp[\"post_treatment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0a1930d-433a-47d1-82b7-56637dbbf4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Imputer(torch.nn.Module):\n",
    "    def __init__(self,config,rnvae):\n",
    "        super(Imputer,self).__init__()\n",
    "        self.impute_loss_weight = config[\"impute_loss_weight\"]\n",
    "        self.imp = utils.make_sequential(len(landmark_cols),config[\"hidden_dim\"],len(transcriptome_cols),config[\"dropout\"])\n",
    "        self.rnvae = rnvae\n",
    "\n",
    "    @classmethod\n",
    "    def make_input(cls, mask):\n",
    "        kg_df = kaggle_joined_df[mask]\n",
    "        trn_df = de_train[mask]\n",
    "        rninp = RNVAE.make_input(kg_df)\n",
    "        trm = trn_df[transcriptome_cols]\n",
    "        for i,inp in enumerate(rninp):\n",
    "            inp[\"transcriptome\"] = torch.tensor(trm.iloc[i].to_numpy(), dtype=torch.float)\n",
    "        return rninp\n",
    "\n",
    "    def forward(self,inp):\n",
    "        fwd = self.rnvae(inp)\n",
    "        trm = self.imp(fwd[\"x_hat\"])\n",
    "        fwd[\"transcriptome\"] = trm\n",
    "        return fwd\n",
    "\n",
    "    def loss_function(self,fwd,inp):\n",
    "        trm_loss = torch.nn.functional.mse_loss(fwd[\"transcriptome\"], inp[\"transcriptome\"])\n",
    "        lossdict = self.rnvae.loss_function(fwd,inp)\n",
    "        lossdict[\"loss\"] += self.impute_loss_weight*trm_loss\n",
    "        lossdict[\"Transcriptome_Loss\"] = trm_loss.detach()\n",
    "        return lossdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "292fd1f0-aea5-4f60-b943-4376884cc608",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 10000/10000 [00:54<00:00, 183.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "918 918\n",
      "tensor(True)\n",
      "cell_type torch.int64 torch.Size([])\n",
      "sm_name torch.int64 torch.Size([])\n",
      "pre_treatment torch.float32 torch.Size([918])\n",
      "post_treatment torch.float32 torch.Size([918])\n",
      "cell_type torch.int64 torch.Size([])\n",
      "sm_name torch.int64 torch.Size([])\n",
      "pre_treatment torch.float32 torch.Size([918])\n",
      "post_treatment torch.float32 torch.Size([918])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bsz = 512\n",
    "lincs_sample = lincs_joined_df.sample(10000)\n",
    "rnvae_inp_new = RNVAE.make_input_new(lincs_sample)\n",
    "rnvae_inp = RNVAE.make_input(lincs_sample,disabletqdm=False)\n",
    "print(len(rnvae_inp_new[0][\"pre_treatment\"]),len(rnvae_inp[0][\"pre_treatment\"]))\n",
    "print((rnvae_inp_new[0][\"pre_treatment\"] == rnvae_inp[0][\"pre_treatment\"]).all())\n",
    "\n",
    "for k, v in rnvae_inp_new[0].items():\n",
    "    print(k,v.dtype,v.shape)\n",
    "\n",
    "for k, v in rnvae_inp[0].items():\n",
    "    print(k,v.dtype,v.shape)\n",
    "\n",
    "rnvae_loader = torch.utils.data.DataLoader(rnvae_inp, batch_size=bsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fed2d2f9-716b-44e3-8379-95ea3ecdaa92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell_type torch.int64\n",
      "sm_name torch.int64\n",
      "pre_treatment torch.float32\n",
      "post_treatment torch.float32\n",
      "transcriptome torch.float32\n",
      "\n",
      "cell_type torch.int64\n",
      "sm_name torch.int64\n",
      "pre_treatment torch.float32\n",
      "post_treatment torch.float32\n",
      "transcriptome torch.float32\n",
      "cell_type torch.int64\n",
      "sm_name torch.int64\n",
      "pre_treatment torch.float32\n",
      "post_treatment torch.float32\n",
      "transcriptome torch.float32\n",
      "\n",
      "cell_type torch.int64\n",
      "sm_name torch.int64\n",
      "pre_treatment torch.float32\n",
      "post_treatment torch.float32\n",
      "transcriptome torch.float32\n",
      "cell_type torch.int64\n",
      "sm_name torch.int64\n",
      "pre_treatment torch.float32\n",
      "post_treatment torch.float32\n",
      "transcriptome torch.float32\n",
      "\n",
      "cell_type torch.int64\n",
      "sm_name torch.int64\n",
      "pre_treatment torch.float32\n",
      "post_treatment torch.float32\n",
      "transcriptome torch.float32\n"
     ]
    }
   ],
   "source": [
    "train_loaders = []\n",
    "eval_loaders = []\n",
    "for fold in fold_to_eval_df:\n",
    "    traind = Imputer.make_input(~make_mask(fold))\n",
    "    for k, v in traind[0].items():\n",
    "        print(k,v.dtype)\n",
    "    print()\n",
    "    train_loaders.append(torch.utils.data.DataLoader(traind, batch_size=bsz))\n",
    "    \n",
    "    evald = Imputer.make_input(make_mask(fold))\n",
    "    for k, v in evald[0].items():\n",
    "        print(k,v.dtype)\n",
    "    eval_loaders.append(torch.utils.data.DataLoader(evald, batch_size=len(evald)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81e19f66-3b3d-40fd-8733-0c6160b845b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch(models):\n",
    "    def _epoch(model,opt,loader):\n",
    "        for batch in loader:\n",
    "            opt.zero_grad()\n",
    "            fwd = model(batch)\n",
    "            loss = model.loss_function(fwd,batch)[\"loss\"]\n",
    "            if torch.isnan(loss):\n",
    "                return loss.detach()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        \n",
    "        return loss.detach()\n",
    "\n",
    "\n",
    "    loss = _epoch(models[\"rnvae\"],models[\"rnvae_opt\"],models[\"rnvae_loader\"])\n",
    "    if torch.isnan(loss):\n",
    "        return loss\n",
    "\n",
    "    loss = _epoch(models[\"imputer\"],models[\"impute_opt\"],models[\"train_loader\"])\n",
    "    if torch.isnan(loss):\n",
    "        return loss\n",
    "\n",
    "    imputer = models[\"imputer\"]\n",
    "    with torch.no_grad():\n",
    "        eval = next(iter(models[\"eval_loader\"]))\n",
    "        fwd = imputer(eval)\n",
    "        # The eval loss we wish to optimize is how well the model\n",
    "        # predicts the full transcriptome.\n",
    "        return imputer.loss_function(fwd,eval)[\"Transcriptome_Loss\"]\n",
    "\n",
    "def make_models(config, input_data, fold):\n",
    "    rnvae = RNVAE(config)\n",
    "    imputer = Imputer(config,rnvae)\n",
    "    return {\n",
    "        \"rnvae\": rnvae,\n",
    "        \"imputer\": imputer,\n",
    "        \"rnvae_opt\": torch.optim.Adam(rnvae.parameters(), lr=config[\"lr_rnvae\"]),\n",
    "        \"impute_opt\": torch.optim.Adam(imputer.parameters(), lr=config[\"lr_imputer\"]),\n",
    "        \"rnvae_loader\": input_data[\"rnvae_loader\"], # There is just one rnvae_loader shared across all folds\n",
    "        \"train_loader\": input_data[\"train_loaders\"][fold],\n",
    "        \"eval_loader\": input_data[\"eval_loaders\"][fold]\n",
    "    }\n",
    "    \n",
    "def train_model(config, input_data):    \n",
    "    def report(epoch,result):\n",
    "        train.report(result)\n",
    "        if epoch % 10 == 0:\n",
    "            print(epoch,result)\n",
    "    \n",
    "    all_models = []\n",
    "    for fold in input_data[\"fold_to_eval_df\"]:\n",
    "        all_models.append(make_models(config, input_data, fold))\n",
    "\n",
    "    for i in range(input_data[\"epochs\"]):\n",
    "        losses = []\n",
    "        for fold in input_data[\"fold_to_eval_df\"]:\n",
    "            losses.append(epoch(all_models[fold]))\n",
    "        \n",
    "        if np.any(np.isnan(losses)):\n",
    "            report(i,{input_data[\"metric\"]: np.nan, \"done\": True})\n",
    "        else:\n",
    "            report(i,{input_data[\"metric\"]: np.mean(losses)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a6172cda-8d6e-4e80-8f1b-4908c94ec968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-10-21 16:21:05</td></tr>\n",
       "<tr><td>Running for: </td><td>00:13:08.96        </td></tr>\n",
       "<tr><td>Memory:      </td><td>6.5/8.0 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=8<br>Bracket: Iter 80.000: -16.019848585128784 | Iter 20.000: -17.922584533691406 | Iter 5.000: -18.51802396774292<br>Logical resource usage: 4.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 7<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name          </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                                                                     </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_22845687</td><td style=\"text-align: right;\">           1</td><td>/Users/laurasisson/ray_results/train_model_2023-10-21_16-07-56/train_model_22845687_11_cell_emb_size=1,dropout=0.6582,hidden_dim=4,impute_loss_weight=0.2249,kld_weight=0.8452,latent_dim=963,lr__2023-10-21_16-10-26/error.txt</td></tr>\n",
       "<tr><td>train_model_f9b5a998</td><td style=\"text-align: right;\">           1</td><td>/Users/laurasisson/ray_results/train_model_2023-10-21_16-07-56/train_model_f9b5a998_12_cell_emb_size=10,dropout=0.3987,hidden_dim=59,impute_loss_weight=6.7682,kld_weight=3.9745,latent_dim=96,lr_2023-10-21_16-10-55/error.txt</td></tr>\n",
       "<tr><td>train_model_eeb3aaf5</td><td style=\"text-align: right;\">           1</td><td>/Users/laurasisson/ray_results/train_model_2023-10-21_16-07-56/train_model_eeb3aaf5_14_cell_emb_size=1,dropout=0.4214,hidden_dim=274,impute_loss_weight=0.2076,kld_weight=0.9336,latent_dim=1075,_2023-10-21_16-11-47/error.txt</td></tr>\n",
       "<tr><td>train_model_d56f80a9</td><td style=\"text-align: right;\">           1</td><td>/Users/laurasisson/ray_results/train_model_2023-10-21_16-07-56/train_model_d56f80a9_17_cell_emb_size=6,dropout=0.5781,hidden_dim=18,impute_loss_weight=7.1018,kld_weight=2.4764,latent_dim=1,lr_i_2023-10-21_16-12-35/error.txt</td></tr>\n",
       "<tr><td>train_model_4ce2b7eb</td><td style=\"text-align: right;\">           1</td><td>/Users/laurasisson/ray_results/train_model_2023-10-21_16-07-56/train_model_4ce2b7eb_20_cell_emb_size=2,dropout=0.9585,hidden_dim=5,impute_loss_weight=2.0832,kld_weight=1.3076,latent_dim=4,lr_im_2023-10-21_16-13-24/error.txt</td></tr>\n",
       "<tr><td>train_model_577b24bb</td><td style=\"text-align: right;\">           1</td><td>/Users/laurasisson/ray_results/train_model_2023-10-21_16-07-56/train_model_577b24bb_23_cell_emb_size=2,dropout=0.1050,hidden_dim=7,impute_loss_weight=0.8929,kld_weight=1.8859,latent_dim=22,lr_i_2023-10-21_16-14-18/error.txt</td></tr>\n",
       "<tr><td>train_model_11d45035</td><td style=\"text-align: right;\">           1</td><td>/Users/laurasisson/ray_results/train_model_2023-10-21_16-07-56/train_model_11d45035_25_cell_emb_size=11,dropout=0.1056,hidden_dim=2,impute_loss_weight=0.7696,kld_weight=3.0119,latent_dim=423,lr_2023-10-21_16-14-45/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name          </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  cell_emb_size</th><th style=\"text-align: right;\">  dropout</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">  impute_loss_weight</th><th style=\"text-align: right;\">  kld_weight</th><th style=\"text-align: right;\">  latent_dim</th><th style=\"text-align: right;\">  lr_imputer</th><th style=\"text-align: right;\">   lr_rnvae</th><th style=\"text-align: right;\">  sm_emb_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">       mse</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_56f3dfce</td><td>TERMINATED</td><td>127.0.0.1:28615</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">0.958979 </td><td style=\"text-align: right;\">          25</td><td style=\"text-align: right;\">            4.27346 </td><td style=\"text-align: right;\">    0.46721 </td><td style=\"text-align: right;\">           5</td><td style=\"text-align: right;\"> 0.000151456</td><td style=\"text-align: right;\">9.6189e-05 </td><td style=\"text-align: right;\">            5</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       384.782  </td><td style=\"text-align: right;\">  19.1383 </td></tr>\n",
       "<tr><td>train_model_4982e562</td><td>TERMINATED</td><td>127.0.0.1:28619</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">0.589091 </td><td style=\"text-align: right;\">         879</td><td style=\"text-align: right;\">            3.72412 </td><td style=\"text-align: right;\">    0.2346  </td><td style=\"text-align: right;\">           5</td><td style=\"text-align: right;\"> 0.0198546  </td><td style=\"text-align: right;\">0.000392642</td><td style=\"text-align: right;\">           14</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">       235.936  </td><td style=\"text-align: right;\">3010.87   </td></tr>\n",
       "<tr><td>train_model_02321c4c</td><td>TERMINATED</td><td>127.0.0.1:28624</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">0.174596 </td><td style=\"text-align: right;\">         439</td><td style=\"text-align: right;\">            0.311833</td><td style=\"text-align: right;\">    4.06219 </td><td style=\"text-align: right;\">          20</td><td style=\"text-align: right;\"> 0.00129081 </td><td style=\"text-align: right;\">0.00107111 </td><td style=\"text-align: right;\">            4</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       760.872  </td><td style=\"text-align: right;\">   7.76841</td></tr>\n",
       "<tr><td>train_model_b5963ae5</td><td>TERMINATED</td><td>127.0.0.1:28630</td><td style=\"text-align: right;\">              4</td><td style=\"text-align: right;\">0.465926 </td><td style=\"text-align: right;\">          55</td><td style=\"text-align: right;\">            1.5513  </td><td style=\"text-align: right;\">    1.02055 </td><td style=\"text-align: right;\">         172</td><td style=\"text-align: right;\"> 0.00212294 </td><td style=\"text-align: right;\">0.0147067  </td><td style=\"text-align: right;\">            6</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.15241</td><td style=\"text-align: right;\"> nan      </td></tr>\n",
       "<tr><td>train_model_461749ab</td><td>TERMINATED</td><td>127.0.0.1:28630</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">0.829947 </td><td style=\"text-align: right;\">         290</td><td style=\"text-align: right;\">            1.35113 </td><td style=\"text-align: right;\">    0.89025 </td><td style=\"text-align: right;\">          98</td><td style=\"text-align: right;\"> 0.00411452 </td><td style=\"text-align: right;\">0.0150033  </td><td style=\"text-align: right;\">            7</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        12.5717 </td><td style=\"text-align: right;\"> nan      </td></tr>\n",
       "<tr><td>train_model_4f60eaa4</td><td>TERMINATED</td><td>127.0.0.1:28630</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">0.758455 </td><td style=\"text-align: right;\">          41</td><td style=\"text-align: right;\">            3.02716 </td><td style=\"text-align: right;\">    0.155665</td><td style=\"text-align: right;\">          15</td><td style=\"text-align: right;\"> 0.00430916 </td><td style=\"text-align: right;\">0.000839257</td><td style=\"text-align: right;\">           12</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       370.934  </td><td style=\"text-align: right;\">  17.9885 </td></tr>\n",
       "<tr><td>train_model_9804af93</td><td>TERMINATED</td><td>127.0.0.1:28645</td><td style=\"text-align: right;\">              9</td><td style=\"text-align: right;\">0.26718  </td><td style=\"text-align: right;\">         750</td><td style=\"text-align: right;\">            0.153273</td><td style=\"text-align: right;\">    0.141419</td><td style=\"text-align: right;\">          35</td><td style=\"text-align: right;\"> 6.41321e-05</td><td style=\"text-align: right;\">0.000668739</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">       276.943  </td><td style=\"text-align: right;\">  19.1318 </td></tr>\n",
       "<tr><td>train_model_9e8282b0</td><td>TERMINATED</td><td>127.0.0.1:28649</td><td style=\"text-align: right;\">              4</td><td style=\"text-align: right;\">0.450362 </td><td style=\"text-align: right;\">          79</td><td style=\"text-align: right;\">            0.934815</td><td style=\"text-align: right;\">    0.137884</td><td style=\"text-align: right;\">         173</td><td style=\"text-align: right;\"> 0.00357777 </td><td style=\"text-align: right;\">0.000309717</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">       111.416  </td><td style=\"text-align: right;\">  18.6728 </td></tr>\n",
       "<tr><td>train_model_81b719dc</td><td>TERMINATED</td><td>127.0.0.1:28654</td><td style=\"text-align: right;\">              3</td><td style=\"text-align: right;\">0.263742 </td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">            0.153157</td><td style=\"text-align: right;\">    0.436029</td><td style=\"text-align: right;\">           7</td><td style=\"text-align: right;\"> 0.000520024</td><td style=\"text-align: right;\">0.157971   </td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        29.9228 </td><td style=\"text-align: right;\"> nan      </td></tr>\n",
       "<tr><td>train_model_447422d9</td><td>TERMINATED</td><td>127.0.0.1:28654</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">0.889989 </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">            0.477095</td><td style=\"text-align: right;\">    2.16713 </td><td style=\"text-align: right;\">         220</td><td style=\"text-align: right;\"> 4.92328e-05</td><td style=\"text-align: right;\">0.0102716  </td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        25.2297 </td><td style=\"text-align: right;\"> nan      </td></tr>\n",
       "<tr><td>train_model_c22b9ad0</td><td>TERMINATED</td><td>127.0.0.1:28755</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">0.405599 </td><td style=\"text-align: right;\">          69</td><td style=\"text-align: right;\">            0.393896</td><td style=\"text-align: right;\">    0.304987</td><td style=\"text-align: right;\">           7</td><td style=\"text-align: right;\"> 0.00985792 </td><td style=\"text-align: right;\">0.114042   </td><td style=\"text-align: right;\">            5</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        19.6442 </td><td style=\"text-align: right;\"> nan      </td></tr>\n",
       "<tr><td>train_model_c1d03dab</td><td>TERMINATED</td><td>127.0.0.1:28619</td><td style=\"text-align: right;\">             13</td><td style=\"text-align: right;\">0.857677 </td><td style=\"text-align: right;\">        1050</td><td style=\"text-align: right;\">            3.44645 </td><td style=\"text-align: right;\">    1.30283 </td><td style=\"text-align: right;\">          40</td><td style=\"text-align: right;\"> 4.54949e-05</td><td style=\"text-align: right;\">0.0457132  </td><td style=\"text-align: right;\">           10</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        22.2496 </td><td style=\"text-align: right;\"> nan      </td></tr>\n",
       "<tr><td>train_model_0b193abd</td><td>TERMINATED</td><td>127.0.0.1:28813</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">0.433509 </td><td style=\"text-align: right;\">         249</td><td style=\"text-align: right;\">            1.65064 </td><td style=\"text-align: right;\">    0.978906</td><td style=\"text-align: right;\">          66</td><td style=\"text-align: right;\"> 0.155231   </td><td style=\"text-align: right;\">0.00240265 </td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        23.5713 </td><td style=\"text-align: right;\"> nan      </td></tr>\n",
       "<tr><td>train_model_b1383bdb</td><td>TERMINATED</td><td>127.0.0.1:28813</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">0.499716 </td><td style=\"text-align: right;\">          56</td><td style=\"text-align: right;\">            0.188645</td><td style=\"text-align: right;\">    4.52247 </td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\"> 0.0126091  </td><td style=\"text-align: right;\">0.000334819</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       299.287  </td><td style=\"text-align: right;\">  17.7693 </td></tr>\n",
       "<tr><td>train_model_3e64523c</td><td>TERMINATED</td><td>127.0.0.1:28850</td><td style=\"text-align: right;\">             14</td><td style=\"text-align: right;\">0.429132 </td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">            0.276281</td><td style=\"text-align: right;\">    0.142242</td><td style=\"text-align: right;\">          72</td><td style=\"text-align: right;\"> 0.0142855  </td><td style=\"text-align: right;\">0.150131   </td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        27.1714 </td><td style=\"text-align: right;\"> nan      </td></tr>\n",
       "<tr><td>train_model_38dfad9e</td><td>TERMINATED</td><td>127.0.0.1:28850</td><td style=\"text-align: right;\">              8</td><td style=\"text-align: right;\">0.0887609</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">            0.585242</td><td style=\"text-align: right;\">    0.332899</td><td style=\"text-align: right;\">          27</td><td style=\"text-align: right;\"> 0.00248468 </td><td style=\"text-align: right;\">0.00010635 </td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        40.9197 </td><td style=\"text-align: right;\">  19.4746 </td></tr>\n",
       "<tr><td>train_model_43e1cee9</td><td>TERMINATED</td><td>127.0.0.1:28944</td><td style=\"text-align: right;\">             11</td><td style=\"text-align: right;\">0.946434 </td><td style=\"text-align: right;\">         352</td><td style=\"text-align: right;\">            0.238447</td><td style=\"text-align: right;\">    0.591797</td><td style=\"text-align: right;\">          48</td><td style=\"text-align: right;\"> 0.154504   </td><td style=\"text-align: right;\">0.00644651 </td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        23.6751 </td><td style=\"text-align: right;\"> nan      </td></tr>\n",
       "<tr><td>train_model_e326c5f7</td><td>TERMINATED</td><td>127.0.0.1:28944</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">0.811496 </td><td style=\"text-align: right;\">         785</td><td style=\"text-align: right;\">            0.996057</td><td style=\"text-align: right;\">    2.13273 </td><td style=\"text-align: right;\">         724</td><td style=\"text-align: right;\"> 0.0957941  </td><td style=\"text-align: right;\">0.115791   </td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        11.8437 </td><td style=\"text-align: right;\"> nan      </td></tr>\n",
       "<tr><td>train_model_22845687</td><td>ERROR     </td><td>127.0.0.1:28654</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">0.658183 </td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">            0.224857</td><td style=\"text-align: right;\">    0.845212</td><td style=\"text-align: right;\">         963</td><td style=\"text-align: right;\"> 0.000553426</td><td style=\"text-align: right;\">9.55111e-05</td><td style=\"text-align: right;\">           10</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_model_f9b5a998</td><td>ERROR     </td><td>127.0.0.1:28649</td><td style=\"text-align: right;\">             10</td><td style=\"text-align: right;\">0.398717 </td><td style=\"text-align: right;\">          59</td><td style=\"text-align: right;\">            6.76818 </td><td style=\"text-align: right;\">    3.97454 </td><td style=\"text-align: right;\">          96</td><td style=\"text-align: right;\"> 0.000132786</td><td style=\"text-align: right;\">0.367734   </td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_model_eeb3aaf5</td><td>ERROR     </td><td>127.0.0.1:28755</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">0.421353 </td><td style=\"text-align: right;\">         274</td><td style=\"text-align: right;\">            0.207644</td><td style=\"text-align: right;\">    0.933629</td><td style=\"text-align: right;\">        1075</td><td style=\"text-align: right;\"> 0.000135028</td><td style=\"text-align: right;\">0.12283    </td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_model_d56f80a9</td><td>ERROR     </td><td>127.0.0.1:28619</td><td style=\"text-align: right;\">              6</td><td style=\"text-align: right;\">0.578123 </td><td style=\"text-align: right;\">          18</td><td style=\"text-align: right;\">            7.10184 </td><td style=\"text-align: right;\">    2.47637 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\"> 0.158758   </td><td style=\"text-align: right;\">0.0516393  </td><td style=\"text-align: right;\">           12</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_model_4ce2b7eb</td><td>ERROR     </td><td>127.0.0.1:28645</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">0.958485 </td><td style=\"text-align: right;\">           5</td><td style=\"text-align: right;\">            2.08318 </td><td style=\"text-align: right;\">    1.30755 </td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\"> 6.09698e-05</td><td style=\"text-align: right;\">0.120048   </td><td style=\"text-align: right;\">           17</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_model_577b24bb</td><td>ERROR     </td><td>127.0.0.1:28850</td><td style=\"text-align: right;\">              2</td><td style=\"text-align: right;\">0.105041 </td><td style=\"text-align: right;\">           7</td><td style=\"text-align: right;\">            0.892928</td><td style=\"text-align: right;\">    1.88593 </td><td style=\"text-align: right;\">          22</td><td style=\"text-align: right;\"> 0.000124351</td><td style=\"text-align: right;\">0.123047   </td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_model_11d45035</td><td>ERROR     </td><td>127.0.0.1:28944</td><td style=\"text-align: right;\">             11</td><td style=\"text-align: right;\">0.105614 </td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">            0.76958 </td><td style=\"text-align: right;\">    3.01186 </td><td style=\"text-align: right;\">         423</td><td style=\"text-align: right;\"> 8.62859e-05</td><td style=\"text-align: right;\">0.000474743</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-21 16:07:57,484\tWARNING worker.py:2058 -- Warning: The actor ImplicitFunc is very large (92 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=28615)\u001b[0m 0 {'mse': 19.236553}\n",
      "\u001b[2m\u001b[36m(train_model pid=28615)\u001b[0m 10 {'mse': 19.445566}\n",
      "\u001b[2m\u001b[36m(train_model pid=28619)\u001b[0m 0 {'mse': 477.25305}\n",
      "\u001b[2m\u001b[36m(train_model pid=28615)\u001b[0m 20 {'mse': 22.055792}\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(train_model pid=28630)\u001b[0m 0 {'mse': 19.179174}\n",
      "\u001b[2m\u001b[36m(train_model pid=28615)\u001b[0m 30 {'mse': 19.190908}\n",
      "\u001b[2m\u001b[36m(train_model pid=28645)\u001b[0m 0 {'mse': 19.181307}\n",
      "\u001b[2m\u001b[36m(train_model pid=28649)\u001b[0m 0 {'mse': 19.151356}\n",
      "\u001b[2m\u001b[36m(train_model pid=28630)\u001b[0m 10 {'mse': 18.967073}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-21 16:10:51,195\tERROR tune_controller.py:2231 -- Could not re-use actor for trial train_model_22845687: Trainable runner reuse requires reset_config() to be implemented and return True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=28615)\u001b[0m 40 {'mse': 19.18001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-21 16:11:06,764\tWARNING util.py:315 -- The `on_step_begin` operation took 0.607 s, which may be a performance bottleneck.\n",
      "2023-10-21 16:11:16,491\tERROR tune_controller.py:2231 -- Could not re-use actor for trial train_model_f9b5a998: Trainable runner reuse requires reset_config() to be implemented and return True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=28615)\u001b[0m 50 {'mse': 19.317554}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-21 16:12:02,045\tERROR tune_controller.py:2231 -- Could not re-use actor for trial train_model_eeb3aaf5: Trainable runner reuse requires reset_config() to be implemented and return True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=28630)\u001b[0m 20 {'mse': 18.778696}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-21 16:12:39,909\tERROR tune_controller.py:2231 -- Could not re-use actor for trial train_model_d56f80a9: Trainable runner reuse requires reset_config() to be implemented and return True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=28615)\u001b[0m 60 {'mse': 19.166344}\n",
      "\u001b[2m\u001b[36m(train_model pid=28624)\u001b[0m 10 {'mse': 17.958761}\n",
      "\u001b[2m\u001b[36m(train_model pid=28630)\u001b[0m 30 {'mse': 18.615685}\n",
      "\u001b[2m\u001b[36m(train_model pid=28813)\u001b[0m 0 {'mse': 19.060581}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-21 16:13:39,681\tWARNING util.py:315 -- The `on_step_begin` operation took 3.571 s, which may be a performance bottleneck.\n",
      "2023-10-21 16:13:42,124\tERROR tune_controller.py:2231 -- Could not re-use actor for trial train_model_4ce2b7eb: Trainable runner reuse requires reset_config() to be implemented and return True.\n",
      "2023-10-21 16:14:09,443\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.808 s, which may be a performance bottleneck.\n",
      "2023-10-21 16:14:09,448\tWARNING util.py:315 -- The `process_trial_result` operation took 1.816 s, which may be a performance bottleneck.\n",
      "2023-10-21 16:14:09,451\tWARNING util.py:315 -- Processing trial results took 1.820 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-10-21 16:14:09,453\tWARNING util.py:315 -- The `process_trial_result` operation took 1.822 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=28850)\u001b[0m 0 {'mse': 19.521143}\n",
      "\u001b[2m\u001b[36m(train_model pid=28615)\u001b[0m 70 {'mse': 19.152044}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-21 16:14:40,146\tERROR tune_controller.py:2231 -- Could not re-use actor for trial train_model_577b24bb: Trainable runner reuse requires reset_config() to be implemented and return True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=28630)\u001b[0m 40 {'mse': 18.4758}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-21 16:14:59,276\tERROR tune_controller.py:2231 -- Could not re-use actor for trial train_model_11d45035: Trainable runner reuse requires reset_config() to be implemented and return True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=28615)\u001b[0m 80 {'mse': 19.145899}\n",
      "\u001b[2m\u001b[36m(train_model pid=28615)\u001b[0m 90 {'mse': 19.15601}\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(train_model pid=28630)\u001b[0m 60 {'mse': 18.254393}\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(train_model pid=28813)\u001b[0m 20 {'mse': 18.163446}\n",
      "\u001b[2m\u001b[36m(train_model pid=28630)\u001b[0m 70 {'mse': 18.16812}\n",
      "\u001b[2m\u001b[36m(train_model pid=28630)\u001b[0m 80 {'mse': 18.095419}\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(train_model pid=28813)\u001b[0m 30 {'mse': 17.912172}\n",
      "\u001b[2m\u001b[36m(train_model pid=28630)\u001b[0m 90 {'mse': 18.034513}\n",
      "\u001b[2m\u001b[36m(train_model pid=28813)\u001b[0m 40 {'mse': 17.848322}\n",
      "\u001b[2m\u001b[36m(train_model pid=28813)\u001b[0m 50 {'mse': 17.817028}\n",
      "\u001b[2m\u001b[36m(train_model pid=28813)\u001b[0m 60 {'mse': 17.806108}\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(train_model pid=28813)\u001b[0m 70 {'mse': 17.781229}\n",
      "\u001b[2m\u001b[36m(train_model pid=28624)\u001b[0m 40 {'mse': 14.459207}\n",
      "\u001b[2m\u001b[36m(train_model pid=28813)\u001b[0m 90 {'mse': 17.768427}\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(train_model pid=28624)\u001b[0m 50 {'mse': 12.429126}\n",
      "\u001b[2m\u001b[36m(train_model pid=28624)\u001b[0m 60 {'mse': 13.805942}\n",
      "\u001b[2m\u001b[36m(train_model pid=28624)\u001b[0m 70 {'mse': 14.043445}\n",
      "\u001b[2m\u001b[36m(train_model pid=28624)\u001b[0m 80 {'mse': 11.972636}\n",
      "\u001b[2m\u001b[36m(train_model pid=28624)\u001b[0m 90 {'mse': 12.067871}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-21 16:21:05,417\tERROR tune.py:1139 -- Trials did not complete: [train_model_22845687, train_model_f9b5a998, train_model_eeb3aaf5, train_model_d56f80a9, train_model_4ce2b7eb, train_model_577b24bb, train_model_11d45035]\n",
      "2023-10-21 16:21:05,419\tINFO tune.py:1143 -- Total run time: 789.29 seconds (788.96 seconds for the tuning loop).\n",
      "2023-10-21 16:21:05,488\tWARNING experiment_analysis.py:205 -- Failed to fetch metrics for 7 trial(s):\n",
      "- train_model_22845687: FileNotFoundError('Could not fetch metrics for train_model_22845687: both result.json and progress.csv were not found at /Users/laurasisson/ray_results/train_model_2023-10-21_16-07-56/train_model_22845687_11_cell_emb_size=1,dropout=0.6582,hidden_dim=4,impute_loss_weight=0.2249,kld_weight=0.8452,latent_dim=963,lr__2023-10-21_16-10-26')\n",
      "- train_model_f9b5a998: FileNotFoundError('Could not fetch metrics for train_model_f9b5a998: both result.json and progress.csv were not found at /Users/laurasisson/ray_results/train_model_2023-10-21_16-07-56/train_model_f9b5a998_12_cell_emb_size=10,dropout=0.3987,hidden_dim=59,impute_loss_weight=6.7682,kld_weight=3.9745,latent_dim=96,lr_2023-10-21_16-10-55')\n",
      "- train_model_eeb3aaf5: FileNotFoundError('Could not fetch metrics for train_model_eeb3aaf5: both result.json and progress.csv were not found at /Users/laurasisson/ray_results/train_model_2023-10-21_16-07-56/train_model_eeb3aaf5_14_cell_emb_size=1,dropout=0.4214,hidden_dim=274,impute_loss_weight=0.2076,kld_weight=0.9336,latent_dim=1075,_2023-10-21_16-11-47')\n",
      "- train_model_d56f80a9: FileNotFoundError('Could not fetch metrics for train_model_d56f80a9: both result.json and progress.csv were not found at /Users/laurasisson/ray_results/train_model_2023-10-21_16-07-56/train_model_d56f80a9_17_cell_emb_size=6,dropout=0.5781,hidden_dim=18,impute_loss_weight=7.1018,kld_weight=2.4764,latent_dim=1,lr_i_2023-10-21_16-12-35')\n",
      "- train_model_4ce2b7eb: FileNotFoundError('Could not fetch metrics for train_model_4ce2b7eb: both result.json and progress.csv were not found at /Users/laurasisson/ray_results/train_model_2023-10-21_16-07-56/train_model_4ce2b7eb_20_cell_emb_size=2,dropout=0.9585,hidden_dim=5,impute_loss_weight=2.0832,kld_weight=1.3076,latent_dim=4,lr_im_2023-10-21_16-13-24')\n",
      "- train_model_577b24bb: FileNotFoundError('Could not fetch metrics for train_model_577b24bb: both result.json and progress.csv were not found at /Users/laurasisson/ray_results/train_model_2023-10-21_16-07-56/train_model_577b24bb_23_cell_emb_size=2,dropout=0.1050,hidden_dim=7,impute_loss_weight=0.8929,kld_weight=1.8859,latent_dim=22,lr_i_2023-10-21_16-14-18')\n",
      "- train_model_11d45035: FileNotFoundError('Could not fetch metrics for train_model_11d45035: both result.json and progress.csv were not found at /Users/laurasisson/ray_results/train_model_2023-10-21_16-07-56/train_model_11d45035_25_cell_emb_size=11,dropout=0.1056,hidden_dim=2,impute_loss_weight=0.7696,kld_weight=3.0119,latent_dim=423,lr_2023-10-21_16-14-45')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/laurasisson/ray_results/train_model_2023-10-21_16-07-56/train_model_02321c4c_3_cell_emb_size=1,dropout=0.1746,hidden_dim=439,impute_loss_weight=0.3118,kld_weight=4.0622,latent_dim=20,lr__2023-10-21_16-08-08\n",
      "CONFIG: {'cell_emb_size': 1, 'dropout': 0.17459607590516624, 'hidden_dim': 439, 'impute_loss_weight': 0.3118326491123, 'kld_weight': 4.062189584858828, 'latent_dim': 20, 'lr_imputer': 0.001290806826201169, 'lr_rnvae': 0.0010711102093205497, 'sm_emb_size': 4}\n",
      "METRICS: {'mse': 7.7684097, 'timestamp': 1697919665, 'done': True, 'training_iteration': 100, 'trial_id': '02321c4c', 'date': '2023-10-21_16-21-05', 'time_this_iter_s': 3.382596969604492, 'time_total_s': 760.8720269203186, 'pid': 28624, 'hostname': 'Lauras-Air', 'node_ip': '127.0.0.1', 'config': {'cell_emb_size': 1, 'dropout': 0.17459607590516624, 'hidden_dim': 439, 'impute_loss_weight': 0.3118326491123, 'kld_weight': 4.062189584858828, 'latent_dim': 20, 'lr_imputer': 0.001290806826201169, 'lr_rnvae': 0.0010711102093205497, 'sm_emb_size': 4}, 'time_since_restore': 760.8720269203186, 'iterations_since_restore': 100, 'checkpoint_dir_name': None, 'experiment_tag': '3_cell_emb_size=1,dropout=0.1746,hidden_dim=439,impute_loss_weight=0.3118,kld_weight=4.0622,latent_dim=20,lr_imputer=0.0013,lr_rnvae=0.0011,sm_emb_size=4'}\n"
     ]
    }
   ],
   "source": [
    "num_samples = 25\n",
    "epochs = 100\n",
    "metric = \"mse\"\n",
    "\n",
    "input_data = {\n",
    "    \"rnvae_loader\": rnvae_loader,\n",
    "    \"train_loaders\": train_loaders,\n",
    "    \"eval_loaders\": eval_loaders,\n",
    "    \"fold_to_eval_df\": fold_to_eval_df,\n",
    "    \"epochs\": epochs,\n",
    "    \"metric\": metric\n",
    "}\n",
    "\n",
    "example_config = {\n",
    "    \"lr_rnvae\": 1e-3,\n",
    "    \"lr_imputer\": 1e-4,\n",
    "    \"dropout\": .1,\n",
    "    \"sm_emb_size\": 64,\n",
    "    \"cell_emb_size\": 32,\n",
    "    \"latent_dim\": 256,\n",
    "    \"hidden_dim\": 512,\n",
    "    \"kld_weight\": 1,\n",
    "    \"impute_loss_weight\": 2,\n",
    "}\n",
    "\n",
    "space = {\n",
    "    \"lr_rnvae\": hp.loguniform(\"lr_rnvae\", -10, -1),\n",
    "    \"lr_imputer\": hp.loguniform(\"lr_imputer\", -10, -1),\n",
    "    \"dropout\": hp.uniform(\"dropout\", 0, 1),\n",
    "    \"sm_emb_size\": scope.int(hp.qloguniform(\"sm_emb_size\", 0, 3, 1)),\n",
    "    \"cell_emb_size\": scope.int(hp.qloguniform(\"cell_emb_size\", 0, 3, 1)),\n",
    "    \"latent_dim\": scope.int(hp.qloguniform(\"latent_dim\", 0, 7, 1)),\n",
    "    \"hidden_dim\": scope.int(hp.qloguniform(\"hidden_dim\", 0, 7, 1)),\n",
    "    \"kld_weight\": hp.loguniform(\"kld_weight\", -2, 2),\n",
    "    \"impute_loss_weight\": hp.loguniform(\"impute_loss_weight\", -2, 2),\n",
    "}\n",
    "\n",
    "train_model(example_config,input_data)\n",
    "mode = \"min\"\n",
    "hyperopt_search = HyperOptSearch(space, metric=\"mse\", mode=mode)\n",
    "scheduler = ASHAScheduler(metric=\"mse\", grace_period=5, mode=mode, max_t=epochs)\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_parameters(train_model, input_data=input_data),\n",
    "    tune_config=tune.TuneConfig(\n",
    "        num_samples=num_samples,\n",
    "        search_alg=hyperopt_search,\n",
    "        scheduler=scheduler\n",
    "    ),\n",
    "    run_config=train.RunConfig(\n",
    "        failure_config=train.FailureConfig(fail_fast=False))\n",
    ")\n",
    "results = tuner.fit()\n",
    "\n",
    "best_result = results.get_best_result(metric, mode=mode)\n",
    "print(best_result.path)\n",
    "print(\"CONFIG:\", best_result.config)\n",
    "print(\"METRICS:\", best_result.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "600ff319-7538-475c-8dbf-b7bc03faaaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mask = make_mask(0) | True\n",
    "all_train = Imputer.make_input(all_mask)\n",
    "all_loader = torch.utils.data.DataLoader(all_train, batch_size=32)\n",
    "\n",
    "submit_data = RNVAE.make_test(test_joined_df)\n",
    "submit_loader = torch.utils.data.DataLoader(submit_data, batch_size=len(submit_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3bcb3e65-60d8-4da6-89bf-131e675ac773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cell_emb_size': 1, 'dropout': 0.17459607590516624, 'hidden_dim': 439, 'impute_loss_weight': 0.3118326491123, 'kld_weight': 4.062189584858828, 'latent_dim': 20, 'lr_imputer': 0.001290806826201169, 'lr_rnvae': 0.0010711102093205497, 'sm_emb_size': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [02:26<00:00,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.1736)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A1BG-AS1</th>\n",
       "      <th>A2M</th>\n",
       "      <th>A2M-AS1</th>\n",
       "      <th>A2MP1</th>\n",
       "      <th>A4GALT</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>AACS</th>\n",
       "      <th>AAGAB</th>\n",
       "      <th>AAK1</th>\n",
       "      <th>...</th>\n",
       "      <th>ZUP1</th>\n",
       "      <th>ZW10</th>\n",
       "      <th>ZWILCH</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZXDA</th>\n",
       "      <th>ZXDB</th>\n",
       "      <th>ZXDC</th>\n",
       "      <th>ZYG11B</th>\n",
       "      <th>ZYX</th>\n",
       "      <th>ZZEF1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.528672</td>\n",
       "      <td>0.318549</td>\n",
       "      <td>0.521696</td>\n",
       "      <td>0.771293</td>\n",
       "      <td>1.164450</td>\n",
       "      <td>0.884213</td>\n",
       "      <td>0.039088</td>\n",
       "      <td>0.527970</td>\n",
       "      <td>-0.104824</td>\n",
       "      <td>0.180595</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.107548</td>\n",
       "      <td>0.202047</td>\n",
       "      <td>0.168787</td>\n",
       "      <td>0.420231</td>\n",
       "      <td>0.739610</td>\n",
       "      <td>0.506831</td>\n",
       "      <td>0.240892</td>\n",
       "      <td>0.206056</td>\n",
       "      <td>-0.084824</td>\n",
       "      <td>-0.098783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.030574</td>\n",
       "      <td>-0.025990</td>\n",
       "      <td>-0.019649</td>\n",
       "      <td>0.041366</td>\n",
       "      <td>-0.017743</td>\n",
       "      <td>-0.081546</td>\n",
       "      <td>-0.123232</td>\n",
       "      <td>0.013412</td>\n",
       "      <td>-0.080594</td>\n",
       "      <td>0.112765</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074695</td>\n",
       "      <td>-0.067768</td>\n",
       "      <td>-0.154886</td>\n",
       "      <td>0.006338</td>\n",
       "      <td>0.156259</td>\n",
       "      <td>0.106656</td>\n",
       "      <td>0.057608</td>\n",
       "      <td>0.048692</td>\n",
       "      <td>0.077000</td>\n",
       "      <td>-0.189186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.523027</td>\n",
       "      <td>0.228987</td>\n",
       "      <td>0.273987</td>\n",
       "      <td>0.232100</td>\n",
       "      <td>0.788171</td>\n",
       "      <td>1.206717</td>\n",
       "      <td>0.020542</td>\n",
       "      <td>0.381870</td>\n",
       "      <td>-0.083118</td>\n",
       "      <td>0.142458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011714</td>\n",
       "      <td>0.137847</td>\n",
       "      <td>0.052237</td>\n",
       "      <td>0.385553</td>\n",
       "      <td>0.558164</td>\n",
       "      <td>0.430852</td>\n",
       "      <td>0.284110</td>\n",
       "      <td>0.172620</td>\n",
       "      <td>-0.097646</td>\n",
       "      <td>-0.050526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.178113</td>\n",
       "      <td>0.071593</td>\n",
       "      <td>0.302333</td>\n",
       "      <td>0.282079</td>\n",
       "      <td>0.409346</td>\n",
       "      <td>0.122760</td>\n",
       "      <td>-0.009732</td>\n",
       "      <td>0.240176</td>\n",
       "      <td>-0.141487</td>\n",
       "      <td>0.052558</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128297</td>\n",
       "      <td>0.098732</td>\n",
       "      <td>-0.013200</td>\n",
       "      <td>0.107470</td>\n",
       "      <td>0.324498</td>\n",
       "      <td>0.194461</td>\n",
       "      <td>0.168248</td>\n",
       "      <td>0.127013</td>\n",
       "      <td>-0.060557</td>\n",
       "      <td>-0.041977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000687</td>\n",
       "      <td>-0.073101</td>\n",
       "      <td>-0.075086</td>\n",
       "      <td>0.064975</td>\n",
       "      <td>0.122777</td>\n",
       "      <td>-0.040922</td>\n",
       "      <td>-0.125111</td>\n",
       "      <td>0.013967</td>\n",
       "      <td>-0.094423</td>\n",
       "      <td>0.113349</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145611</td>\n",
       "      <td>-0.079895</td>\n",
       "      <td>-0.190313</td>\n",
       "      <td>-0.027026</td>\n",
       "      <td>0.200238</td>\n",
       "      <td>0.141642</td>\n",
       "      <td>-0.009138</td>\n",
       "      <td>0.023725</td>\n",
       "      <td>0.097237</td>\n",
       "      <td>-0.227018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>1.628841</td>\n",
       "      <td>0.780313</td>\n",
       "      <td>1.896007</td>\n",
       "      <td>2.646558</td>\n",
       "      <td>4.904253</td>\n",
       "      <td>3.166702</td>\n",
       "      <td>0.464417</td>\n",
       "      <td>1.569543</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>-0.110998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030858</td>\n",
       "      <td>0.838427</td>\n",
       "      <td>0.812316</td>\n",
       "      <td>1.221225</td>\n",
       "      <td>2.348658</td>\n",
       "      <td>1.466492</td>\n",
       "      <td>0.850378</td>\n",
       "      <td>0.304324</td>\n",
       "      <td>-0.248923</td>\n",
       "      <td>0.104876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0.310764</td>\n",
       "      <td>0.046431</td>\n",
       "      <td>0.510666</td>\n",
       "      <td>0.601503</td>\n",
       "      <td>1.854848</td>\n",
       "      <td>0.558109</td>\n",
       "      <td>0.044957</td>\n",
       "      <td>0.061032</td>\n",
       "      <td>-0.282235</td>\n",
       "      <td>0.111371</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.311963</td>\n",
       "      <td>-0.007312</td>\n",
       "      <td>-0.150231</td>\n",
       "      <td>-0.264160</td>\n",
       "      <td>0.619928</td>\n",
       "      <td>0.109334</td>\n",
       "      <td>0.122755</td>\n",
       "      <td>-0.029426</td>\n",
       "      <td>-0.130031</td>\n",
       "      <td>-0.010530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>1.764767</td>\n",
       "      <td>0.504572</td>\n",
       "      <td>0.873535</td>\n",
       "      <td>2.365860</td>\n",
       "      <td>10.343506</td>\n",
       "      <td>6.479660</td>\n",
       "      <td>0.502747</td>\n",
       "      <td>1.515960</td>\n",
       "      <td>0.444920</td>\n",
       "      <td>-0.819880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.551924</td>\n",
       "      <td>0.135300</td>\n",
       "      <td>1.401836</td>\n",
       "      <td>0.654832</td>\n",
       "      <td>3.497616</td>\n",
       "      <td>1.634827</td>\n",
       "      <td>0.872020</td>\n",
       "      <td>0.341745</td>\n",
       "      <td>-0.901023</td>\n",
       "      <td>0.585574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>3.101876</td>\n",
       "      <td>1.583784</td>\n",
       "      <td>2.687714</td>\n",
       "      <td>4.606878</td>\n",
       "      <td>12.430525</td>\n",
       "      <td>7.394593</td>\n",
       "      <td>0.538450</td>\n",
       "      <td>2.536995</td>\n",
       "      <td>-0.288898</td>\n",
       "      <td>-0.163105</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.201641</td>\n",
       "      <td>1.429180</td>\n",
       "      <td>1.243499</td>\n",
       "      <td>2.115982</td>\n",
       "      <td>5.056717</td>\n",
       "      <td>2.590979</td>\n",
       "      <td>1.249429</td>\n",
       "      <td>0.134950</td>\n",
       "      <td>-0.653481</td>\n",
       "      <td>0.165899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>0.770334</td>\n",
       "      <td>0.131853</td>\n",
       "      <td>1.368790</td>\n",
       "      <td>1.417684</td>\n",
       "      <td>4.453613</td>\n",
       "      <td>1.702669</td>\n",
       "      <td>0.313289</td>\n",
       "      <td>0.517054</td>\n",
       "      <td>-0.245450</td>\n",
       "      <td>0.160695</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.391975</td>\n",
       "      <td>0.320167</td>\n",
       "      <td>0.260397</td>\n",
       "      <td>-0.281400</td>\n",
       "      <td>1.500652</td>\n",
       "      <td>0.522748</td>\n",
       "      <td>0.539774</td>\n",
       "      <td>0.260404</td>\n",
       "      <td>-0.278045</td>\n",
       "      <td>0.389040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>255 rows × 18211 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         A1BG  A1BG-AS1       A2M   A2M-AS1      A2MP1    A4GALT      AAAS  \\\n",
       "id                                                                           \n",
       "0    0.528672  0.318549  0.521696  0.771293   1.164450  0.884213  0.039088   \n",
       "1   -0.030574 -0.025990 -0.019649  0.041366  -0.017743 -0.081546 -0.123232   \n",
       "2    0.523027  0.228987  0.273987  0.232100   0.788171  1.206717  0.020542   \n",
       "3    0.178113  0.071593  0.302333  0.282079   0.409346  0.122760 -0.009732   \n",
       "4    0.000687 -0.073101 -0.075086  0.064975   0.122777 -0.040922 -0.125111   \n",
       "..        ...       ...       ...       ...        ...       ...       ...   \n",
       "250  1.628841  0.780313  1.896007  2.646558   4.904253  3.166702  0.464417   \n",
       "251  0.310764  0.046431  0.510666  0.601503   1.854848  0.558109  0.044957   \n",
       "252  1.764767  0.504572  0.873535  2.365860  10.343506  6.479660  0.502747   \n",
       "253  3.101876  1.583784  2.687714  4.606878  12.430525  7.394593  0.538450   \n",
       "254  0.770334  0.131853  1.368790  1.417684   4.453613  1.702669  0.313289   \n",
       "\n",
       "         AACS     AAGAB      AAK1  ...      ZUP1      ZW10    ZWILCH  \\\n",
       "id                                 ...                                 \n",
       "0    0.527970 -0.104824  0.180595  ... -0.107548  0.202047  0.168787   \n",
       "1    0.013412 -0.080594  0.112765  ... -0.074695 -0.067768 -0.154886   \n",
       "2    0.381870 -0.083118  0.142458  ...  0.011714  0.137847  0.052237   \n",
       "3    0.240176 -0.141487  0.052558  ... -0.128297  0.098732 -0.013200   \n",
       "4    0.013967 -0.094423  0.113349  ... -0.145611 -0.079895 -0.190313   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "250  1.569543  0.001372 -0.110998  ...  0.030858  0.838427  0.812316   \n",
       "251  0.061032 -0.282235  0.111371  ... -0.311963 -0.007312 -0.150231   \n",
       "252  1.515960  0.444920 -0.819880  ...  0.551924  0.135300  1.401836   \n",
       "253  2.536995 -0.288898 -0.163105  ... -0.201641  1.429180  1.243499   \n",
       "254  0.517054 -0.245450  0.160695  ... -0.391975  0.320167  0.260397   \n",
       "\n",
       "        ZWINT      ZXDA      ZXDB      ZXDC    ZYG11B       ZYX     ZZEF1  \n",
       "id                                                                         \n",
       "0    0.420231  0.739610  0.506831  0.240892  0.206056 -0.084824 -0.098783  \n",
       "1    0.006338  0.156259  0.106656  0.057608  0.048692  0.077000 -0.189186  \n",
       "2    0.385553  0.558164  0.430852  0.284110  0.172620 -0.097646 -0.050526  \n",
       "3    0.107470  0.324498  0.194461  0.168248  0.127013 -0.060557 -0.041977  \n",
       "4   -0.027026  0.200238  0.141642 -0.009138  0.023725  0.097237 -0.227018  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "250  1.221225  2.348658  1.466492  0.850378  0.304324 -0.248923  0.104876  \n",
       "251 -0.264160  0.619928  0.109334  0.122755 -0.029426 -0.130031 -0.010530  \n",
       "252  0.654832  3.497616  1.634827  0.872020  0.341745 -0.901023  0.585574  \n",
       "253  2.115982  5.056717  2.590979  1.249429  0.134950 -0.653481  0.165899  \n",
       "254 -0.281400  1.500652  0.522748  0.539774  0.260404 -0.278045  0.389040  \n",
       "\n",
       "[255 rows x 18211 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_input_data = {\n",
    "    \"rnvae_loader\": rnvae_loader,\n",
    "    \"train_loaders\": [all_loader],\n",
    "    \"eval_loaders\": [all_loader],\n",
    "    \"fold_to_eval_df\": fold_to_eval_df,\n",
    "}\n",
    "\n",
    "best_models = make_models(best_result.config,best_input_data,0)\n",
    "print(best_result.config)\n",
    "# Because we trained the models on a cross-validation split, we want to train one final model\n",
    "# across all data available.\n",
    "\n",
    "loss = 0\n",
    "for _ in tqdm.tqdm(range(best_result.metrics[\"training_iteration\"])):\n",
    "    loss = epoch(best_models)\n",
    "print(loss)\n",
    "\n",
    "with torch.no_grad():\n",
    "    submitbatch = next(iter(submit_loader))\n",
    "    # This is the most elegant line of python ever written.\n",
    "    y_pred = best_models[\"imputer\"](submitbatch)[\"transcriptome\"]\n",
    "\n",
    "\n",
    "submission = pd.DataFrame(y_pred, columns=transcriptome_cols, index=id_map.index)\n",
    "display(submission)\n",
    "submission.to_csv('submissions/rnvae.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
